{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24b8c836-2eed-48af-84a5-cc3dfc5f0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,random,time,itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from types import SimpleNamespace as _NS\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e9e33a8-a0ae-422c-8932-0967bd58082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG=_NS(\n",
    "    pooled_pt=\"/Users/xshan/Desktop/PLM/model/esmC_600m_training5.pt\",\n",
    "    score_pt=\"/Users/xshan/Desktop/PLM/model/esmC_600m_Pagg.pt\",\n",
    "    out_csv=\"testrun_scores_multiseed.csv\",\n",
    "\n",
    "    epochs=10,\n",
    "    adapter_lr=0.0001,\n",
    "    adapter_hidden=512,\n",
    "    adapter_middle=128,\n",
    "    adapter_latent=64,\n",
    "    batch_size=512,\n",
    "    n_pp_diff=60000,\n",
    "    n_nn_diff=1000,\n",
    "    n_pn_diff=1000,\n",
    "    margin=1,\n",
    "    loss_scale=1000,\n",
    "    seeds=[2,3,4,5,6],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "mapping={\n",
    "    \"G01\": \"amidotransferase\",\n",
    "    \"G02\": \"methyltransferase\",\n",
    "    \"G03\": \"methyltransferase\",\n",
    "    \"G04\": \"methyltransferase\",\n",
    "    \"G05\": \"monooxygenase\",\n",
    "    \"G06\": \"monooxygenase\",\n",
    "    \"G07\": \"monooxygenase\",\n",
    "    \"G08\": \"monooxygenase\",\n",
    "    \"G09\": \"prenyltransferase\",\n",
    "    \"G10\": \"prenyltransferase\",\n",
    "    \"G11\": \"adenylation\",\n",
    "    \"G12\": \"dehydrogenase\",\n",
    "    \"G13\": \"dehydrogenase\",\n",
    "    \"G14\": \"dehydrogenase\",\n",
    "    \"test\": \"test\",\n",
    "    \"amidotransferase\": \"amidotransferase\",\n",
    "    \"methyltransferase\": \"methyltransferase\",\n",
    "    \"monooxygenase\": \"monooxygenase\",\n",
    "    \"prenyltransferase\": \"prenyltransferase\",\n",
    "    \"adenylation\": \"adenylation\",\n",
    "    \"dehydrogenase\": \"dehydrogenase\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45578e6d-9776-4812-b0ee-5f28174409c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self,in_dim,hidden=256,middle=128,latent=64,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1=nn.Linear(in_dim,hidden)\n",
    "        self.fc2=nn.Linear(hidden,middle)\n",
    "        self.mu=nn.Linear(middle,latent)\n",
    "        self.act=nn.ReLU(inplace=True)\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        h1=self.drop(self.act(self.fc1(x)))\n",
    "        h2=self.drop(self.act(self.fc2(h1)))\n",
    "        z=self.mu(h2)\n",
    "        return z\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self,margin=1.0,scale=10000):\n",
    "        super().__init__()\n",
    "        self.margin=margin\n",
    "        self.scale=scale\n",
    "    def forward(self,z1,z2,labels):\n",
    "        d=F.pairwise_distance(z1,z2,p=2)\n",
    "        pos_loss=d**2\n",
    "        neg_loss=F.relu(self.margin-d)**2\n",
    "        loss=torch.where(labels==1,pos_loss,neg_loss)\n",
    "        return self.scale * loss.mean()\n",
    "\n",
    "class IndexPairDataset(Dataset):\n",
    "    def __init__(self,pairs):\n",
    "        self.pairs=pairs\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self,k):\n",
    "        return self.pairs[k]\n",
    "\n",
    "def collate_fetch(batch,X_train_device,X_train):\n",
    "    ii=torch.tensor([b[0] for b in batch],dtype=torch.long)\n",
    "    jj=torch.tensor([b[1] for b in batch],dtype=torch.long)\n",
    "    yy=torch.tensor([b[2] for b in batch],dtype=torch.long)\n",
    "    x1=X_train.index_select(0,ii.to(X_train_device))\n",
    "    x2=X_train.index_select(0,jj.to(X_train_device))\n",
    "    return x1,x2,yy.to(X_train_device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(encoder,X,device,batch_size=128):\n",
    "    encoder.eval()\n",
    "    feats=[]\n",
    "    for i in range(0,len(X),batch_size):\n",
    "        xb=X[i:i+batch_size].to(device)\n",
    "        z=encoder(xb).cpu().numpy()\n",
    "        feats.append(z)\n",
    "    return np.concatenate(feats,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f77d3a7f-c199-4f8f-8d48-effdff357510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rng(seed):\n",
    "    return np.random.default_rng(seed)\n",
    "\n",
    "def _canon(i,j):\n",
    "    return (i,j) if i<j else (j,i)\n",
    "\n",
    "def build_PN_same_all(group_to_pos,group_to_neg):\n",
    "    out=[]\n",
    "    shared=sorted(set(group_to_pos) & set(group_to_neg))\n",
    "    for g in shared:\n",
    "        for i in group_to_pos[g]:\n",
    "            for j in group_to_neg[g]:\n",
    "                out.append((i,j))\n",
    "    return out\n",
    "\n",
    "def sample_PP_diff_stream(group_to_pos,k,seed):\n",
    "    if k<=0:\n",
    "        return []\n",
    "    rng=_rng(seed)\n",
    "    pos_groups=[g for g, v in group_to_pos.items() if v]\n",
    "    if len(pos_groups)<2:\n",
    "        return []\n",
    "    seen=set()\n",
    "    out=[]\n",
    "    max_attempts=max(10000,20*k)\n",
    "    attempts=0\n",
    "    while len(out)<k and attempts < max_attempts:\n",
    "        g1,g2=rng.choice(pos_groups,size=2,replace=False)\n",
    "        i=rng.choice(group_to_pos[g1])\n",
    "        j=rng.choice(group_to_pos[g2])\n",
    "        key=_canon(i,j)\n",
    "        if key in seen:\n",
    "            attempts+=1\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(key)\n",
    "    return out\n",
    "\n",
    "def sample_NN_diff_stream(group_to_neg,k,seed):\n",
    "    if k<=0:\n",
    "        return []\n",
    "    rng=_rng(seed)\n",
    "    neg_groups=[g for g,v in group_to_neg.items() if v]\n",
    "    if len(neg_groups)<2:\n",
    "        return []\n",
    "    seen=set()\n",
    "    out=[]\n",
    "    max_attempts=max(10000,20*k)\n",
    "    attempts=0\n",
    "    while len(out)<k and attempts<max_attempts:\n",
    "        g1,g2=rng.choice(neg_groups,size=2,replace=False)\n",
    "        i=rng.choice(group_to_neg[g1])\n",
    "        j=rng.choice(group_to_neg[g2])\n",
    "        key=_canon(i,j)\n",
    "        if key in seen:\n",
    "            attempts+=1\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(key)\n",
    "    return out\n",
    "\n",
    "def sample_PN_diff_stream(group_to_pos,group_to_neg,k,seed):\n",
    "    if k<=0:\n",
    "        return []\n",
    "    rng = _rng(seed)\n",
    "    pos_groups=[g for g,v in group_to_pos.items() if v]\n",
    "    neg_groups=[g for g,v in group_to_neg.items() if v]\n",
    "    if not pos_groups or not neg_groups:\n",
    "        return []\n",
    "    seen=set()\n",
    "    out=[]\n",
    "    max_attempts=max(20000,30*k)\n",
    "    attempts=0\n",
    "    while len(out) < k and attempts < max_attempts:\n",
    "        gp=rng.choice(pos_groups)\n",
    "        gn=rng.choice(neg_groups)\n",
    "        if gp==gn:\n",
    "            attempts+=1\n",
    "            continue\n",
    "        i=rng.choice(group_to_pos[gp])\n",
    "        j=rng.choice(group_to_neg[gn])\n",
    "        key=(i,j)\n",
    "        if key in seen:\n",
    "            attempts +=1\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(key)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8ff0458-fdb7-4080-a2ce-ec9875cf8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_seed(cfg,seed,verbose=False):\n",
    "    t_all0=time.time()\n",
    "    set_seed(seed)\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    blob=torch.load(CFG.pooled_pt,weights_only=True)\n",
    "    X_all,ids_all=blob[\"X\"],blob[\"ids\"]\n",
    "    N,D=X_all.shape\n",
    "\n",
    "    def parse_label_group(sid):\n",
    "        parts=sid.split(\"_\")\n",
    "        return (1 if parts[0].lower()==\"positive\" else 0, parts[1])\n",
    "\n",
    "    labels,groups=zip(*(parse_label_group(sid) for sid in ids_all))\n",
    "    meta_df=pd.DataFrame({\"id\":ids_all,\"label\":labels,\"group\":groups})\n",
    "    meta_df[\"group\"]=meta_df[\"group\"].map(mapping)\n",
    "\n",
    "    df_train=meta_df\n",
    "    X_train=X_all[df_train.index].to(device)\n",
    "    y_train=torch.tensor(df_train[\"label\"].to_numpy(),dtype=torch.long)\n",
    "    groups_train=df_train[\"group\"].tolist()\n",
    "\n",
    "    group_to_pos,group_to_neg=defaultdict(list),defaultdict(list)\n",
    "    for i,(lab,grp) in enumerate(zip(y_train.numpy(),groups_train)):\n",
    "        (group_to_pos if lab==1 else group_to_neg)[grp].append(i)\n",
    "\n",
    "    PN_same_all=build_PN_same_all(group_to_pos,group_to_neg)\n",
    "    PP_diff_sel=sample_PP_diff_stream(group_to_pos,CFG.n_pp_diff,seed)\n",
    "    NN_diff_sel=sample_NN_diff_stream(group_to_neg,CFG.n_nn_diff,seed+1)\n",
    "    PN_diff_sel=sample_PN_diff_stream(group_to_pos,group_to_neg,CFG.n_pn_diff,seed+2)\n",
    "    \n",
    "    def warn_short(name,got,want):\n",
    "        if got<want:\n",
    "            print(f\"[warn] {name}: requested {want}, got {got}\",flush=True)\n",
    "    warn_short(\"PP_diff\",len(PP_diff_sel),CFG.n_pp_diff)\n",
    "    warn_short(\"NN_diff\", len(NN_diff_sel), CFG.n_nn_diff)\n",
    "    warn_short(\"PN_diff\", len(PN_diff_sel), CFG.n_pn_diff)\n",
    "\n",
    "    pairs=([(i,j,-1) for (i,j) in PN_same_all]+\n",
    "           [(i,j,1) for (i,j) in PP_diff_sel]+\n",
    "           [(i,j,1) for (i,j) in NN_diff_sel]+\n",
    "           [(i,j,-1) for (i,j) in PN_diff_sel])\n",
    "\n",
    "    _rng_local=np.random.default_rng(seed+3)\n",
    "    _rng_local.shuffle(pairs)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"[seed {seed}] pairs: \"\n",
    "            f\"PN_same={len(PN_same_all)}, \"\n",
    "            f\"PP_diff={len(PP_diff_sel)}, \"\n",
    "            f\"NN_diff={len(NN_diff_sel)}, \"\n",
    "            f\"PN_diff={len(PN_diff_sel)}, \"\n",
    "            f\"total={len(pairs)}\",\n",
    "            flush=True\n",
    "        )\n",
    "\n",
    "    pair_ds=IndexPairDataset(pairs)\n",
    "    pair_loader=DataLoader(\n",
    "        pair_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=lambda b: collate_fetch(b,device,X_train),\n",
    "    )\n",
    "\n",
    "    encoder=Adapter(\n",
    "        in_dim=X_train.shape[1],\n",
    "        hidden=CFG.adapter_hidden,\n",
    "        middle=CFG.adapter_middle,\n",
    "        latent=CFG.adapter_latent\n",
    "    ).to(device)\n",
    "\n",
    "    optim=torch.optim.AdamW(encoder.parameters(),lr=CFG.adapter_lr)\n",
    "    criterion=ContrastiveLoss(margin=CFG.margin,scale=CFG.loss_scale)\n",
    "\n",
    "    for ep in range(CFG.epochs):\n",
    "        encoder.train()\n",
    "\n",
    "        total_loss=0.0\n",
    "        n_batches=0\n",
    "\n",
    "        for x1,x2,target in pair_loader:\n",
    "            z1,z2=encoder(x1),encoder(x2)\n",
    "            loss=criterion(z1,z2,target.long())\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss+=loss.item()\n",
    "            n_batches+=1\n",
    "\n",
    "        epoch_loss=total_loss/max(1,n_batches)\n",
    "\n",
    "        print(f\"    Epoch {ep+1}/{cfg['epochs']} | ContrastiveLoss = {epoch_loss:.4f}\")\n",
    "\n",
    "    Xz_train=extract_features(encoder,X_train,device,batch_size=128)\n",
    "\n",
    "    clf=make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(kernel=\"linear\",C=1.0,class_weight=\"balanced\",probability=True,random_state=seed)\n",
    "    )\n",
    "        \n",
    "    clf.fit(Xz_train,y_train.numpy())\n",
    "\n",
    "    return encoder,clf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f69f6-206a-445b-afbd-7622230bce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_score=torch.load(CFG.score_pt,weights_only=True)\n",
    "X_score_raw,ids_score=blob_score[\"X\"],blob_score[\"ids\"]\n",
    "base_cfg = {\n",
    "    \"pooled_pt\": CFG.pooled_pt,\n",
    "    \"epochs\": CFG.epochs,\n",
    "    \"adapter_lr\": CFG.adapter_lr,\n",
    "    \"adapter_hidden\": CFG.adapter_hidden,\n",
    "    \"adapter_middle\": CFG.adapter_middle,\n",
    "    \"adapter_latent\": CFG.adapter_latent,\n",
    "    \"batch_size\": CFG.batch_size,\n",
    "    \"n_pp_diff\": CFG.n_pp_diff,\n",
    "    \"n_nn_diff\": CFG.n_nn_diff,\n",
    "    \"n_pn_diff\": CFG.n_pn_diff,\n",
    "    \"margin\": CFG.margin,\n",
    "    \"loss_scale\": CFG.loss_scale,\n",
    "}\n",
    "\n",
    "all_seed_scores={}\n",
    "\n",
    "for seed in CFG.seeds:\n",
    "    encoder,clf=run_train_seed(base_cfg,seed,verbose=CFG.verbose)\n",
    "    device=next(encoder.parameters()).device\n",
    "    X_score=X_score_raw.to(device)\n",
    "    Xz_score=extract_features(encoder,X_score,device,batch_size=128)\n",
    "\n",
    "    scores=clf.decision_function(Xz_score)\n",
    "    all_seed_scores[seed]=scores\n",
    "\n",
    "df_score=pd.DataFrame({\"id\":list(ids_score)})\n",
    "for seed in CFG.seeds:\n",
    "    df_score[f\"seed_{seed}\"]=all_seed_scores[seed]\n",
    "\n",
    "df_score=df_score.set_index(\"id\")\n",
    "df_score.to_csv(CFG.out_csv)\n",
    "print(f\"\\nSaved scores for {len(df_score)} sequences and {len(CFG.seeds)} seeds to: {CFG.out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79eeb99-4d7a-4090-ab90-f16914f60693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
